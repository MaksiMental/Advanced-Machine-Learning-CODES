{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 2.4: MLP in Tensorflow\n",
    "\n",
    "ITU KSADMAL1KU-NLP - Advanced Machine Learning for NLP in KCS 2024\n",
    "\n",
    "by Stefan Heinrich, Bertram Højer, Christian H. Rasmussen, & material by Kevin Murphy.\n",
    "\n",
    "All info and static material: https://learnit.itu.dk/course/view.php?id=3024579\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "This notebook is a prototypical blueprint for Deep Learning frameworks, usually following four steps: \n",
    "- Data loading and preprocessing (often including Exploratory Data Analysis (EDA))\n",
    "- Building a model by using the Tensorflow or PyTorch API\n",
    "- Training a model (including initialising) until termination (often: convergence)\n",
    "- Analysing the model (often including various steps to achieve interpretability of the model)\n",
    "\n",
    "In Advanced Machine Learning course, we will detail these steps but often revisit these basic framework steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "odJ0Jm89tWiL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# @title ##### just check for a reasonable recent installed tensorflow version.\n",
    "# you can ignore this block\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. DNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4gxhcMwIkWhE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# @title #### Import dependencies\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from IPython import display\n",
    "#import os\n",
    "#import time\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "# TODO: in the future use tensorflow_datasets>4.9.3 when available due to: https://github.com/openvinotoolkit/datumaro/issues/1189\n",
    "\n",
    "np.random.seed(0)  # test with different seeds (for re-running everything)!"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the data"
   ],
   "metadata": {
    "id": "6tqWT2tNEpzx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ##### Variant one: load the dataset from raw data\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
    "x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
    "\n",
    "# Create dataset object:\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# The raw images are of type tf.uint8, while the model expects tf.float32. \n",
    "# Therefore, you need to normalize images.\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., tf.cast(label, tf.int8)\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Some meta information:\n",
    "print(\"Number of original training examples:\", ds_train.cardinality().numpy())\n",
    "print(\"Number of original test examples:\", ds_test.cardinality().numpy())\n",
    "print(\"Input representation: \", ds_train.element_spec[0])\n",
    "print(\"Output representation: \", ds_train.element_spec[1])\n",
    "\n",
    "figure = plt.figure(figsize=(10, 5))\n",
    "cols, rows = 4, 2\n",
    "for i, (img, label) in enumerate(ds_train.take(cols*rows)):\n",
    "  figure.add_subplot(rows, cols, i+1)\n",
    "  plt.title(f\"Class {label}\")\n",
    "  plt.axis(\"off\")\n",
    "  plt.imshow(img[:,:,0], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# As you fit the dataset in memory, cache it before shuffling for a better performance.\n",
    "ds_train, ds_test = ds_train.cache(), ds_test.cache()\n",
    "\n",
    "# For true randomness, you can set the shuffle buffer to the full dataset size.\n",
    "ds_train = ds_train.shuffle(ds_train.cardinality().numpy())\n",
    "\n",
    "# Batch elements of the dataset after shuffling to get unique batches at each epoch.\n",
    "ds_train, ds_test = ds_train.batch(batch_size), ds_test.batch(batch_size)\n",
    "\n",
    "# It is good practice to end the data loading pipeline by prefetching for performance.\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE) \n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n"
   ],
   "metadata": {
    "id": "9PtT2i0aXrFx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ##### Variant two: load prepared dataset object\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "# TFDS provide images of type tf.uint8, while the model expects tf.float32. Therefore, you need to normalize images.\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., tf.cast(label, tf.int8)\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Some meta information:\n",
    "print(\"Number of original training examples:\", ds_train.cardinality().numpy())\n",
    "print(\"Number of original test examples:\", ds_test.cardinality().numpy())\n",
    "print(\"Input representation: \", ds_train.element_spec[0])\n",
    "print(\"Output representation: \", ds_train.element_spec[1])\n",
    "\n",
    "# Some examples:\n",
    "figure = plt.figure(figsize=(10, 5))\n",
    "cols, rows = 4, 2\n",
    "for i, (img, label) in enumerate(ds_train.take(cols*rows)):\n",
    "  figure.add_subplot(rows, cols, i+1)\n",
    "  plt.title(f\"Class {label}\")\n",
    "  plt.axis(\"off\")\n",
    "  plt.imshow(img[:,:,0], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# As you fit the dataset in memory, cache it before shuffling for a better performance.\n",
    "ds_train, ds_test = ds_train.cache(), ds_test.cache()\n",
    "\n",
    "# For true randomness, you can set the shuffle buffer to the full dataset size.\n",
    "ds_train = ds_train.shuffle(ds_train.cardinality().numpy())\n",
    "\n",
    "# Batch elements of the dataset after shuffling to get unique batches at each epoch.\n",
    "ds_train, ds_test = ds_train.batch(batch_size), ds_test.batch(batch_size)\n",
    "\n",
    "# It is good practice to end the data loading pipeline by prefetching for performance.\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE) \n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ],
   "metadata": {
    "id": "ZmicfYt3IvGY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Build the model"
   ],
   "metadata": {
    "id": "2V065zsVE8j5"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dBHJvIfWkkrH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train the model"
   ],
   "metadata": {
    "id": "Q8kInmO1FPME"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define hyperparameters\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "epochs = 5\n",
    "\n",
    "# Setup for training\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, \n",
    "                                    momentum=momentum)\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]  # a set of metrics!\n",
    "                                    \n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=criterion,\n",
    "              metrics=metrics)\n",
    "\n",
    "# Training loop\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=epochs\n",
    ")"
   ],
   "metadata": {
    "id": "y1Cjvke4OOmQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Analyse the model"
   ],
   "metadata": {
    "id": "_CroH6MmFoKh"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NEdpVpqakqSW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# @title ##### Prediction and visualisation\n",
    "# Overall accuracy\n",
    "train_loss, train_acc = model.evaluate(ds_train)\n",
    "print('Train accuracy:', train_acc)\n",
    "test_loss, test_acc = model.evaluate(ds_test)\n",
    "print('Test accuracy:', test_acc)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Derive an histogram over the 10 classes \n",
    "# by counting over the predictions on the test data\n",
    "\n",
    "hist = {k: {l:0 for l in range(10)} for k in range(10)}\n",
    "\n",
    "for images, labels in ds_test:\n",
    "  outputs = model.predict(images, verbose=0)\n",
    "  pred = np.argmax(outputs, axis=1)\n",
    "  ground_truth = labels.numpy()\n",
    "  for k in range(len(outputs)):\n",
    "    hist[ground_truth[k]][pred[k]] += 1\n"
   ],
   "metadata": {
    "id": "9N9M5tpBcuF9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D1JpMMhSrBcn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "for idx, (key, val) in enumerate(hist.items()):\n",
    "  ax = fig.add_subplot(3, 4, idx+1)\n",
    "  ax.bar(list(val.keys()), val.values(), color='r')\n",
    "  ax.set_title(f\"Prediction for {key}\")\n",
    "  ax.set_xticks(range(0,10))\n",
    "  ax.set_ylim(0, 1200)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
